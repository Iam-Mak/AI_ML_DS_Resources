## Evaluation Overview

### Putting together a visualization require lot of moving parts...

- Assess the **domain problem**  the visualization solves

  - Do the stakeholders concur with your assumptions?

- Determine the **data** and **tasks** your users have

  - Can people build the knowledge they need?

- Choose the right **encodings** for your domain, data, and tasks
  -  Can people see the patterns they need correctly?

- Build an algorithm to map the data to the encodings

  - Does the algorithm perform correctly?

- Design interactions to explore and refine data

  - Can people quickly and intuitively interact with the data?

### Insight-Based Evaluation

- Provide the tool or system to people and evaluate its utility by understanding what that tool enables them to do
- Similar to ethnography-make grounded observations about what people discover in their data
- Good insights are complex, deep, qualitative, unexpected, & relevant

### Experimental Evaluation

- Run a controlled study to measure quickly/accurately/efficiently people complete tasks using different visualizations
- Measure how people complete a specific set of tasks under different conditions using a set of established and measurable criteria
- More precise than insights, but less about the domain. Need a baseline.

### Insights are

![image](https://user-images.githubusercontent.com/92245436/157913368-ee812eb1-f8db-4a5f-94ae-c2a2e9227f24.png)

### Types of Insights

Knowledge-building insight:

- Discovering insight, gaining insight, and providing insight

- Insight as a substance, that accumulates over time and could be measured/quantified

Spontaneous insight:

- Experiencing insight, having an insight, or a moment of insight

- Insight as a discrete event, that occurs at a specific moment in time and could be observed

### Metrics for Insight-Based Evaluation

- Time to Insight

- Number of Insights

- Importance of Insights

- Depth of Insights

-  System adoption rate

## Qualitative Evaluation

### Common Qualitative Evaluation Technique 

- **Systematic Surveys:** Read and assemble knowledge on your target topic and existing solutions

- **Semistructured Interviews:** Have a conversation with collaborators stemming from preplanned questions

- **Thinkalouds:** Work with collaborators while using your solution on their own data

- **Journaling:** Ask collaborators to use your tool and note their observations, including what works and what doesn't

### Systematic Surveys

Read and assemble knowledge on your target topic and existing solutions

Understand your target problem and assess existing solutions to discover their shortcomings

Used to:

Understand the problem and prior solutions

Determine what expertise is necessary for a holistic solution

Spec key tasks, datasets, and other considerations

### Semistructured Interviews

Have a conversation with collaborators stemming from preplanned questions

Create a series of questions you can't answer on your own and go talk with collaborators about them

Used to:

Figure out the problems that experts see as most impactful

Decide who will best serve in which roles

Verify your understanding of the target problem Get focused, fluid feedback about what is and isn't working

### Thinkaloud Studies

Work with collaborators while using your solution on their own data

Your collaborators use the tool and narrate their actions and observations. You can note these and immediately address confusions, proposal alternatives, etc.

Like a semistructured interview, but focus is on the visualization solution

Used to:

Establish limits in current solutions and key workflows Get direct feedback on low-fi prototypes grounded in design Identify strengths and weaknesses of the current design


### Journaling/Diary Studies

Ask collaborators to use your tool and note their observations, including what works and what doesn't

You completely remove yourself from the discussion and let the tool speak for itself

The order in which insights arise show the importance of those insights and ho workflows cause them to "snowball"

Used to:

Give your collaborators the tool and let them play with it. Make sure to ask them to document things like interesting findings or confusing settings/bugs with screenshots!

## Experimental Design 

### Experimental Evaluation

Run a controlled study to measure quickly/accurately/efficiently people complete tasks using different visualizations

Measure how people complete a specific set of tasks under different conditions using a set of established and measurable criteria.

More precise than insights, but less about the domain. Need a baseline.

### How to design an experiment: Setting Things Up

1) Form a specific question you want to answer about your vis

2) Generate a set of falsifiable hypotheses about that question

3) Determine your independent (what you change) and dependent (what you measure) variables

4) Build your stimuli & experimental infrastructure (e.g., task framing, how will people complete tasks, data collection, etc.)

### Parts of an Experiment

- Experimental Tasks are what people will do
- Experimental Stimuli are the specific items (generally visualizations) people use to complete the task
- Independent Variables are the different conditions that we want to test (e.g., visualization type)
- Dependent Variables are the things we want to measure (e.g., accuracy)
- Control Variables are things that might affect your results if you don't do something to restrict or randomize them (e.g., the visualized data)

## Trade-Offs in Evaluation 
###  High Level Trade-Offs
![image](https://user-images.githubusercontent.com/92245436/157924723-694c5513-cf8b-47ca-9f3b-1573d755f0f0.png)

### Two Forms of Evaluation
- **Formative Evaluation:** Judge the utility of a visualization as you're developing it (i.e., as it is forming)
- **Summative Evaluation:** Judge the utility of a visualization after you've developed it (i.e., in summary/conclusion)
